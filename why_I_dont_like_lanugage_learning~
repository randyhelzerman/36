/*
\documentclass{book}[9pt]
%\oddsidemargin  0.5in
%\evensidemargin 0.0in
%\textwidth      7in
%\headheight     0.0in
%\topmargin      0.0in
%\textheight     8in

\usepackage[matrix,curve,arrow,tips,frame]{xy}
\usepackage{bar}
\usepackage{epsfig}
\usepackage{verbatim}
\usepackage{makeidx}         % allows index generation
\usepackage{multicol}        % used for the two-column index

\newenvironment{code}%
{\small \verbatim}%
{\endverbatim \large}

\newtheorem{theorem}{Theorem}
\newtheorem{prop}{Proposition}
\newtheorem{axiom}{Axiom}
\newtheorem{definition}{Definition}

\makeindex

\begin{document}

\author{Randall A. Helzerman}
\title{The 36 Chambers of SHRDLU}
\maketitle

\frontmatter

\tableofcontents

\mainmatter

\chapter{Introductory Justifications}

This chapter consists of a bunch of sales jobs--basically its purpose
is to convince you that two propositions are true:
\begin{enumerate}
\item This book is worth spending your {\em money} on to {\em buy} it.
\item This book is worth spending your {\em time} on to {\em understand} it.
\end{enumerate}
\noindent I'll approach this by trying to answer some questions you may be
bringing to this text.

\section{Why arn't you covering language \newline learning or statistical grammars?}

A book on learning languages would be very interesting, but research efforts
in natural language learning today really arn't focused on learning language
as children learn language.  Rather, they are focused on a very different
problem--the induction of statistical parsers from corpora consisting of parse
trees for sentences.  This has been the dominant paradigm now for around 13
years or so (at the time of this writing.  By the standards the researchers
have set for themselves (insert names of metrics here) there's been
lots of progress, but by any reasonable measure, the results are rather 
megre.  Its  not at all obvious that this line of reasearch is going anywhere.

But even if it {\em could} be convincingly pulled off, it {\em still} would
shed no light on how children actually learn language, or on how adults
actually process language, because nobody learns languages from corpora of
parse trees.  Its a problem invented by academics for academics.

And even if you are more interested in saving money than you are in solving
interesting problems, there really is no economic justification for it.  The
promise is that instead of needing a highly-skilled (and therefore expensive)
natural language engineer to manually create grammars, why not just have a
program automatically learn it from parsed corpora?  Sounds great, but lets
not forget that it takes just as highly-skilled (and therefore just as
expensive) natural langauge engineers to produce parsed corpora.  Its actually
far more expensive to create an extensive corpus (and ensure that it is
reasonably error free) than it is to write a wide coverage grammar for a
language.  Microsoft, for example, has extremely wide-coverage grammars for
all the world's major languages, for use in Microsoft Word's grammar
checker--all lovingly hand crafted.

Syntactic theory, after all, is actually the {\em best} understood aspect of
natural language processing--and it certainly isn't what is holding us back.
One motivation for writing this book is to try to bring the attention back to
actual, not artifactual, problems.

\section{Why are you using Prolog instead of (insert favorite language here)}

In this reguard, I think it would be interesting to consider the example of
the book ``Paradigms of AI Programming'' by Peter Norvig, which is, in the
opinion of this author, the best book on programming ever written.  In his
book, Norvig describes in detail several natural language processing programs,
with varying capabilities.  When looking at these programs, an interesting
pattern emerges: any program which needs more than the most basic powers of
analyzing the structure of sentences is written in prolog.  Norvig's preferred
programming languge is LISP, but on the way to natural language processing,
Norvig finds it useful to {\em first} implement prolog in LISP and {\em then}
implement his natural language processing programs on top of prolog.

This is by no means an isolated case.  Carl Hewitt's invention of planner, for
example, or (even more pertinently for our present purposes) Terry Winograd's
SHRDLU.  Because both of these authors emphasized the {\em doing} part of
understanding language (e.g. to understand ``the block'' go and {\em find} a
block in the database) it might seem strange to cite these as examples of
using prolog as an abstraction layer.  However, Hewitt has emphasized that
prolog is really a version of planner, and therefore SHRDLU (which was also
written in a version of planner called MICROPLANNER) very much exemplifies
this strategy of using a prolog-esque layer of abstraction on top of LISP.

This of course, is obvious only in hindsite. The whole LISP/prolog dichotomy
is a false dichotomy.  No matter what programming language you use, you will
find it useful to structure your natural language processing programs such
that they have many levels of abstraction on top of assembly language, and one
of these levels of abstraction is gunna look a lot like prolog.  Since this is
inevitable anyways, we might as well start not from assembly language, or even
LISP, but from prolog.  We can thereby help ourselves to over 30 years worth
of prolog research, which has produced very fast and efficient prolog
compilers, as well as libraries, programming techniques and methodologies,
and a welth of practical know-how in constructing and maintaining programs.
Well begun is half done.

\section{Why all these chambers??}


\chapter{Running, Fixing, and Enhancing this Program}

\section{SWI Prolog}

\subsection{Declarations, etc}


\begin{code}
*/

:- op(870,xfy,[=>]).
:- op(880,xfy,[<=>]).
:-op(900,fx,not).

:-discontiguous(np/6).
:-discontiguous(defunkify/3).
:-discontiguous(case/2).
:-discontiguous(case/2).
:-discontiguous(funky/1).
:-discontiguous(kind/3).
:-discontiguous(word/1).
:-discontiguous(mood/3).
:-discontiguous(process_logical_form/2).
:-discontiguous(sense/3).
:-discontiguous(sense/4).
:-discontiguous(voice/3).
:-discontiguous(sense/5).
:-discontiguous(sense/6).
:-discontiguous(subcat/3).
:-discontiguous(subcat/4).
:-discontiguous(grammatical_number/3).
:-discontiguous(grammatical_person/3).
:-discontiguous(gender/2).
:-discontiguous(category/2).
:-discontiguous(category/3).
:-discontiguous(category/4).
:-discontiguous(behavior/3).
:-discontiguous(verb_base/2).
:-discontiguous(verb_form/5).
:-discontiguous(nc_transform/3).
:-discontiguous(prove/3).
:-discontiguous(quantifier/3).
:-discontiguous(root_word/2).
:-discontiguous(s/3).
:-discontiguous(conjugation/2).
:-discontiguous(thing_range/3).
:-discontiguous(thing_super/2).

:-dynamic(axiom/1).
:-dynamic(dial/3).

/*
\end{code}

\subsection{Not predicate}

\begin{code}
*/

not F --> F, !, { fail }.
not _ --> [].


/*
\end{code}

\chapter{The First Chamber: Prolog}

\section{Difference Lists}

\section{Basic manoevers in handling variables}

In prolog, X == Y is true iff X and Y the same variable, or that one
is an alias of the other.

\section{Double Negation in order not to bind variables trick}

\chapter{The Second Chamber: Tokenization and Parsing}

The tokenizer is a list-to-list transducer which takes the raw input list and
converts it into something which is easier to write a parser for.  Because it
is a list transducer, we use difference lists in order to support
constant-time concatenation.

A limitation of the tokenizer is that it assumes ascii input.  It would have
to be reworked if we were to support Unicode.

The tokenizer supplies two high-level predicates: snarf\_line/1 and chunks/4.
The predicate snarf\_line/1 just keeps reading characters until an end line
has been reached.  The predicate chunks/4 takes a list of characters (such as
that supplied by snarf\_line/1) and breaks it into "chunks".  Chunks
correspond roughly to words and punctuation, with caveats as noted below.
Generally, we follow the lead of the PENN Treebank project \cite{marcus93} on
tokenization.

\section{Getting a line of input from stdin}

\begin{code}
*/

get_sentence(S) :-
        snarf_line(L1),
        chunks(_Dummy,S,L1,_L2).

snarf_line(L) :-
        %  Get a character.
        get0(C),
        
        % process it.
        ( (C = 10) ->
            L = []
        ;
            snarf_line(Cs),
            L = [C|Cs]
        ).

/*
\end{code}

\section{Chunks}

Now, we break up the input list into "chunks":

\begin{code}
*/

chunk([A|L]) -->
        optional_white_space,
        apostrophe(A),
        chunk(L).

chunk(W) -->
        optional_white_space,
        word(W).

chunk(W) -->
        optional_white_space,
        numeral(W).

chunk([W]) -->
        optional_white_space,
        non_chunking_punctuation(W).

/*
\end{code}

\subsection{Whitespace}

Chunks are seperated by whitespace, so we have to specify what counts as
whitespace:

\begin{code}
*/

white_space([H|T]) -->
        white_space_char(H),
        (
          white_space(T)
        ;
          end_white_space(T)
        ).

white_space_char(H) -->
        (
          space_char(H)
        ;
          tab_char(H)
        ).

space_char(32) --> [32].
tab_char(9)    --> [9].

end_white_space([]) --> not white_space_char(_H).

/*
\end{code}

We use this trick for optional white space:

\begin{code}
*/

optional_white_space --> white_space(_S),!.
optional_white_space -->[].

/*
\end{code}

\subsection{Recognizing Letters}

\begin{code}
*/

letter(H) -->
        (
          lower_case_letter(H)
        ;
          upper_case_letter(H)
        ;
          underscore(H)
        ).


lower_case_letter(H) -->
        [H],
        { H >= 97  },
        { H =< 122 }.

upper_case_letter(H) -->
        [H],
        { H >= 65 },
        { H =< 90 }.

underscore(95) --> [95].

/*
\end{code}

\subsection{Words}

The next step is to group segments of letters into words.

\begin{code}
*/

end_word([]) --> not letter(_H).
word([H|T]) -->
        letter(H),
        ( word(T)
        ; end_word(T)
        ).


/*
\end{code}

\subsection{Numerals}

In the same way, we can group digits and numbers:

\begin{code}
*/

digit(H) -->
        [H],
        { H >= 48 },
        { H =< 57 }.

end_numeral([]) --> not digit(_H).
numeral([H|T]) -->
        digit(H),
        (
          numeral(T)
        ;
          end_numeral(T)
        ).

/*
\end{code}

\subsection{Punctuation}

Words can also be seperated by punctuation:

\begin{code}
*/

punctuation(C) -->
        apostrophe(C)
    ;
        non_chunking_punctuation(C).


non_chunking_punctuation(C) -->
        comma(C)
    ;
        semicolon(C)
    ;
        period(C)
    ;
        exclamation_mark(C)
    ;
        question_mark(C)
    ;
        hyphen(C).

comma(44) --> [44].

semicolon(59) --> [59].

apostrophe(39) --> [39].

period(46) --> [46].

exclamation_mark(33) --> [33].

question_mark(63) --> [63].

hyphen(45) --> [45].


/*
\end{code}

\subsection{Defunkification}

So-called "funky" words like "gonna" replaced by  "gon na"

\begin{code}
*/

funky(gunna).
funky(woulda).
funky(coulda).
funky(shoulda).
funky(something).
funky(somethings).

defunkify(gunna,[gun,na|X],X).
defunkify(woulda,[would,a|X],X).
defunkify(coulda,[could,a|X],X).
defunkify(shoulda,[should,a|X],X).
defunkify(something,[some,thing|X],X).
defunkify(somethings,[some,things|X],X).

/*      
\end{code}

\noindent The reason we break things up like this is explained on the Penn
treebank web page, "This tokenization allows us to analyze each component
separately, so (for example) "I" can be in the subject Noun Phrase while "'m"
is the head of the main verb phrase."

The following arn't funky in the sense of the Penn treebank, but since we
already have the machinerty there, it is convenient to treat them as funky.

\begin{code}
*/

funky(table).
defunkify(table,[w_table|X],X).

/*
\end{code}

\subsection{Putting the chunks together}

The following is really interesting...because a single chunk can induce more
than one word in the resulting tokenized stream, we need to use append.  To
make that fast, we need to use difference lists.  This makes the code much
harder to understand...

\begin{code}
*/


chunks(_H,[]) --> not chunk(_W).
chunks(H,T1) -->
        optional_white_space,
        chunk(H1),
        { name(H2,H1) },
        { ( funky(H2) ->
              defunkify(H2,T1,T2)
          ;
              T1 = [H2|T2]
          )
        },
        
        chunks(H,T2).
/*
\end{code}


\chapter{The Third Chamber: Grammars for Natural Language}

\section{Skeleton Grammar of English}

\begin{code}

s --> np, vp.

np --> det, noun.

det --> [a].

noun --> [block].

vp --> verb, adjective.

verb --> [is].

adjective --> [red].
adjective --> [green].

\end{code}

\section{Reusing parts of the grammar}

\begin{code}

s(assertion) --> 
    np, 
    vp([],[]).

s(yn_question) --> 
    verb([],[],W),
    np, 
    vp([gap(W)],[]).

np --> det, noun.

det --> [a].

noun --> [block].

vp(P1,P2) --> verb(P1,P2,_), adjective.


verb([],[],is) --> [is].
verb([gap(W)],[],W) --> [W].


adjective --> [red].
adjective --> [green].

\end{code}

\chapter{The Fourth Chamber: Existential Quantification and Predication}
\label{language_game_0}
In this chapter, we'll  create a very small language--small enough that we can
{\em completely} implement it so that we can:
\begin{enumerate}
\item Parse an input sentence which is an assertion or query.
\item If its an assertion compile it to Horn clauses.
\item If it is a query, compile it to a query against a horn clause database.
\item Take the results of any such query and express them in English.
\end{enumerate}
\noindent In particular, we want to be able to have the following
conversation with the computer:
\begin{verbatim}
a block is blue.
ok.

is a block blue?
yes.

is a block red?
no.

a block is red.
ok.

is a block red?
yes.
\end{verbatim}
\noindent This is, no doubt, not the most scintilating of conversations, but
the important point is that we cover {\em every} part of NLP in that we have
solutions--vestigial and elementary solutions, to be sure--but solutions
nontheless to all phases of a NLP read-eval-print loop.  This will be a secure
foundation we can use to elaborate upon.

\section{Grammar}

\begin{code}

sentence(Mood,L) -> s(Mood,L).

s(assertion,L) --> 
    np(X^Q^L),
    vp([],[], X^Q),
    ['.'].

s(yn_question,L) --> 
    verb([],[],W),
    np(X^Q^L), 
    vp([gap(W)],[], X^Q),
    ['?'].

np(X^Q^L) --> det(X^R^Q^L), noun(X^R).

det(X^R^Q^exists(X,and(R,Q)))--> [a].

noun(X^block(X)) --> [block].

vp(P1,P2,L) --> verb(P1,P2,_), adjective(L).


verb([],[],is) --> [is].
verb([gap(W)],[],W) --> [].


adjective(X^red(X)) --> [red].
adjective(X^green(X)) --> [green].

\end{code}

\section{Compiling to Horn Clauses}

\subsection{Skolemization}

\begin{code}
*/

gensym(Root, Atom) :-
        name(Root,Name1),
        get_num(Root,Num),
        name(Num,Name2),
        append(Name1,Name2,Name),
        name(Atom,Name).

get_num(Root,Num1) :-
        retract(current_num(Root,Num)),!,
        Num1 is Num + 1,
        asserta(current_num(Root,Num1)).

get_num(Root,1) :- asserta(current_num(Root,1)).

skolemise(X) :- gensym(sk,X).

/*
\end{code}

\subsubsection{Nute-Covington Transform}

\begin{code}

nc_transform(F,Cs) :- nc_transform(F,Cs,[]).

nc_transform(exists(X,R), H,T) :- !,
        skolemise(X),
        nc_transform(R,H,T).

nc_transform(and(R,S), H,T) :-!,
        nc_transform(R, H,I),
        nc_transform(S, I,T).

nc_transform(F, [axiom(F)|T],T).

\end{code}

\section{Compling for query}

\subsection{Lloyd-Topor Transform}

\begin{code}

lt_transform(exists(_X,R), R1) :- !,
        lt_transform(R,R1).

lt_transform(and(P,Q), (P1,Q1)) :- !,
        lt_transform(P,P1),
        lt_transform(Q,Q1).

lt_transform(F,F).

\end{code}

\section{Dialog}

\begin{code}

dialog :-
        % remove previous resuls
        retractall(axiom(_)),
        
        read_eval_loop.

read_eval_loop :-
        get_sentence(S),
        ( S=[bye,'.'] ->
            true
        ;
            process_sentence(S),
            read_eval_loop
        ).

process_sentence(S) :-
        % parse the sentence
        ( sentence(Mood,L,S,[]) ->
             % see how to process it
             process_logical_form(Mood,L)
        ;
             write('couldn\'t understand this:'),
             writeln(S)
        ).


% if this is an assertion, run the nute-covington transform and assert
process_logical_form(assertion,L) :-
    nc_transform(L,Fs),
    assert_all(Fs).

assert_all([]).
    
assert_all([F|Fs]) :-
    assert(F),
    assert_all(Fs).

% if this is a query, run the lloyd-topor transform and query
process_logical_form(yn_question,L) :-
    lt_transform(L,Q),
    
    (  prove(Q) ->
        writeln('yes')
     ; 
        writeln('no')
    ).

\end{code}

\section{Theorem Prover}

\begin{code}

prove((A,B)) :-
    prove(A),
    prove(B).

prove(A) :- axiom(A).

\end{code}

\chapter{The Fifth Chamber: Dialog and Database Update}

We already know how to handle simple existential quantification, for sentences
like:
\begin{verbatim}
A block is green.
\end{verbatim}
\noindent But there are many other ways in which exitential quantification is
expressed in English.  The goal of this chapter is to explain how to make a
program which can engage in a conversation like the following.  We should be
able to input sentences like:
\begin{verbatim}
There is a block.
The block is green.
There is a red bock.
\end{verbatim}
\noindent and then the computer shoulde be able to answer questions like:
\begin{verbatim}
Is there a block?
yes.

Is there a table?
no.

Is there a blue block?
no.

Is there a red block?
yes.
\end{verbatim}

\section{Problems to Solve}

How should we handle senteces like:
\begin{verbatim}
There is a block.
\end{verbatim}
\noindent  We might get a clue by considering a sentence like:
\begin{verbatim}
A block is there.
\end{verbatim}
\noindent Which sounds like a verbal pointing, or {\em ostension}.  In other
words, if you say ``A block is {\em there},'' you are saying, of a particular
spatio-temporal region, that it is a block.  For the present, we'll treat such
verbal ostensions as adverbs modifying the verb ``to be.''

But the modifications to the grammar are relatively easy next to our next
problem--that of database update.

\section{Grammar}

\begin{code}

sentence(Mood,L) --> s(Mood,L).

% the block is green.
s(assertion,L) -->
    np(X^Q^L),
    vp([],[], X^Q),
    ['.'].

% there is a block.
s(assertion,L) -->
    advp([],[],AvpF),
    verb([],[],W),
    np(X^Q^L),
    vp([gap(W),gap(advp,AvpF)],[], X^Q),
    ['.'].



% is the block green?
s(yn_question,L) -->
    verb([],[],W),
    np(X^Q^L), 
    vp([gap(W)],[], X^Q),
    ['?'].

% is there a block?
s(yn_question,L) -->
    verb([],[],W),
    advp([],[],AvpF),
    np(X^Q^L), 
    vp([gap(W),gap(advp,AvpF)],[], X^Q),
    ['?'].


np(X^Q^L) --> det(X^R^Q^L), noun(X^R).
np(X^Q^L) --> det(X^R^Q^L), adjective(X^A), noun(X^B), { R=and(A,B) }.


det(X^R^Q^exists(X,and(R,Q)))--> [a].
det(X^R^Q^exists(X,and(R,Q)))--> [the].


noun(X^block(X)) --> [block].


% is green.
vp(P1,P2,L) --> verb(P1,P2,_), adjective(L).

% is there.
vp(P1,P3,L) --> verb(P1,P2,_), advp(P2,P3,L).


verb([],[],is) --> [is].
verb([gap(W)|P],P,W) --> [].


adjective(X^red(X)) --> [red].
adjective(X^green(X)) --> [green].


advp(P,P, F) --> adv(F).
advp([gap(advp,F)|P],P, F) --> [].


adv(_^true) --> [there].

\end{code}

\section{Compiling to Horn Clauses}

The above grammar takes a sentence like:
\begin{verbatim}
There is a block.
\end{verbatim}
\noindent and creates the following logical form:
\begin{verbatim}
exists(X,and(block(X),true)).
\end{verbatim}
\noindent which the NC transform of the previous chapter
would translate into:
\begin{verbatim}
[axiom(block(sk1)), axiom(true)]
\end{verbatim}
\noindent We can insert special code to ignore the axiom(true),
and assert block(sk1).  But then what should we do with the next
sentence input by the user?
\begin{verbatim}
The block is green.
\end{verbatim}
\noindent If we were to just follow the same proceedure,
we would get another list of atoms to assert:
\begin{verbatim}
[axiom(block(sk2)), axiom(green(sk2))]
\end{verbatim}
\noindent But this really isn't what we want at all!  First of all, this seems
to indicate that there are two blocks, sk1 and sk2.  However, this
conversation isn't talking about two blocks, its talking about one block which
we're updating our information about.

So we'll have to change how we compile our logical form into horn clauses, and
we'll have to make a more sophisticated version of updating the database.  As
a point of departure here, we'll take our inspiration from what Quine calls
the ``Maxim of minimum mutilation,'' that is, when faced with new incommming
information, a reasonable thing to do is change your current database to cover
the new data, but change it as little as possible.  For this example, if we
already know that sk1 is a block:
\begin{verbatim}
[axiom(block(sk1))]
\end{verbatim}
\noindent and we get the input sentence ``the block is green,'' we
would just add one more literal to our database, asserting that 
ski is green:
\begin{verbatim}
[axiom(block(sk1)), axiom(green(sk1))]
\end{verbatim}
\noindent Seems easy enough--just match up the incomming info with
the previous info, and add the delta.  However, consider if we input
another sentence:
\begin{verbatim}
There is a red block.
\end{verbatim}
\noindent and follow the same proceedure, we'd get
a database which looks like this:
\begin{verbatim}
[axiom(block(sk1)), axiom(green(sk1)), axiom(red(sk1))]
\end{verbatim}
\noindent Which seems wrong.  What went wrong is that some sentences update
information about previously known objects, and some sentences tell us about
the existance of new objects.  So how can we tell which?  Well, one clue is
that (all parts of) an object can't be two colors at the same time.  So we
follow a broadly Thomistic maxim ``whenever you find a contradiction, make a
distinction.'' \footnote{I got this from Richard Rorty \cite{rorty2000}}
Instead of interpreting the user as saying something {\em contradictory} about
one block, we want the program to interpret the user as saying something {\em
coherent} about two blocks.

Of course, this makes a big assumption--that the user, most of the time at
least, actually speaks coherently!  This assumption, (known as the ``Principle
of Charity,'') plays an pivital role in the philosophy of Quine and Donald
Davidson.  The construction of this program can be taken as experimental
evidence that they are correct--the principle of charity is an essential
presupposition for the possiblity of communication between agents.  The
philosophical ramifications of this are many and profound, and the reader is
encouraged to pursue them.  However, our purposes in this book are more
engineering than philosophy, we'll just help ourselves to these philosophical
results, acknowledge them in passing, and get back to programming.

The problem we need to solve now is this: how do we know that
\begin{verbatim}
[red(sk1), green(sk1)]
\end{verbatim}
\noindent is contradictory?  Notice that this is not of the
form $(A \wedge \neg A)$, so it isn't {\em flatly} contradictory
\footnote{It was the realization of this that persuaded Wittgenstein that 
he needed to abandon the views on logical atomism he proposed in
his {\em Tractatus} \cite{monk}}.
In order to generate the contradiction, we need to have a background
theory about colors and about how they are mutually exclusive.

The kicker is that horn clause logic really doesn't {\em directly} represent
this sort of mutual exclusivity.  The solution known to prolog folklore for
this is to introduce a new predicate, ``false'', and have the conjunction of
mutually contradictory atoms imply false, like this:
\begin{code}
false :- green(X), red(X)
\end{code}
\noindent and then use negation as failure to ensure
that ``false'' can't be derived.

\subsection{Theorem Prover}

So lets start by writing a routing to detect mutually exclusive atoms.  We
want a routine which will take a list of atomic formulas like:
\begin{verbatim}
[block(sk1), green(sk1), red(sk1)]
\end{verbatim}
\noindent succeed if that list is noncontradictory, and fail if it is.

We will do this by adopting our theorem prover to be able to draw conclusions
not just from asserted axioms, but also from a list of input atomic formulas.
As usual, this is just an application of skeletons and techniques: the
skeleton is the vanilla theorem prover, and the technique is to add another
variable which is the list of atomic formlas: \begin{code} */ prove((A,B), R)
:- prove(A, R), prove(B, R).

prove(A, R) :-
    clause((A :-B)),
    prove(B,R).

prove(A,_R) :- axiom(A).

prove(A,R) :- member(A,R).

/*
\end{code}
\noindent then we need some world knowledge 
\begin{code}
% colors are mutuallly exlcusive
clause((false :- green(X), red(X))).
clause((false :- green(X), blue(X))).
clause((false :- red(X), blue(X))).

% so are blockhood and tablehood
clause((false :- block(X), table(X))).
\end{code}
\noindent Sharp-eyed readers can already see a problem with this technique,
don't worry, we'll present a comprehensive solution later.  The upside is that
it is now quite easy to write our desired predicate:
\begin{code}
coherent(L) :- not prove(false,L).
\end{code}

\subsection{Updating the Database}

The first modification we have to make to the tranform clauses is that we
don't want to immediately skolimze the existentially quantified variable to
{\em constants} anymore.  The reason is that a formula like:
\begin{verbatim}
exists(X,block(X))
\end{verbatim}
\noindent might be talking about a block we already know about.  So instead,
we just want to accumulate the existentially quantified variables, and later
try to match them up, if possible, with pre-existing objects.

So using the the c\_transform clause as a skeleton, we add a difference list
to hold the existentially quantified variables we encounter:

\begin{code}
c_transform(F,Cs,Vs) :- c_transform(F,Cs,[], Vs,[]).

c_transform(exists(X,R), CH,CT, [X|VH],VT) :- !,
        c_transform(R, CH,CT, VH,VT).

c_transform(and(R,S), CH,CT, VH,VT) :-!,
        c_transform(R, CH,CI, VH,VI),
        c_transform(S, CI,CT, VI,VT).

c_transform(true, CT,CT, V,V) :- !.
c_transform(F, [F|CT],CT, V,V).
\end{code}
\noindent in addition, the last clause has been modified so that it simply
skips atoms of the form ``axiom(true).''

\section{The Maxim of Minimum Mutilation}

For each of the new existentially-quantified variables, we have to make a
decision--either it refers either to an already-known object, or it refers to
a new object.  To make it easy to find out which, we'll change how we store
the database.  In addition to asserting axioms, we'll also assert two lists.
One list will be a list of constants:
\begin{verbatim}
things([sk1, sk4, sk7,..]).
\end{verbatim} 
\noindent which are the items we've already postulated.  Another list will be
a list of the atomic formulas we hold true of those objects:
\begin{verbatim}
atoms([block(sk1), table(sk4), green(sk4), table(sk7), ...]).
\end{verbatim}
\noindent So the idea is that when we get in the atomic formulas of the new
sentence:
\begin{verbatim}
[block(X), green(X)]
\end{verbatim}
\noindent we'll see if we can match up the variable X with any of the
pre-existing objects without generating a contradictory database.  If we
can't, then we'll follow the Maxim of Thomas, and postulate a new object for
X.
\begin{code}
mmm_update(Vs, Os, Fs, Db1,Db3) :-
	generate_domains(Vs,Os,Fs,Db1, Doms),
	find_simultainously_consistent_assignment(Doms,Fs,Db1,Db2),
	sort(Db2,Db3).

generate_domains([], _Os,_Fs,_Db1, []).
generate_domains([V|Vs], Os,Fs,Db1, [domain(V,Pos)|Doms]) :-
     find_formulas_about_variable(Fs,V,Vfs),
     append(Vfs,Db1,F1s),
     find_possible_values(Os,V,F1s, Pos),
     generate_domains(Vs, Os,F1s,Db1, Doms).

find_formulas_about_variable([],_V,[]).
find_formulas_about_variable([F|Fs],V,R) :-
     F=..[_Functor|Args],
     ( variable_member(V,Args) ->
          R = [F|Vfs]
     ;
          R=Vfs
     ),
     find_formulas_about_variable(Fs,V,Vfs).

variable_member(V,[VT|Vs]) :-
    ( 
        V==VT
    ; 
        variable_member(V,Vs)
    ).


find_possible_values(Os,V,Fs,Pos) :-
    find_possible_preexisting_values(Os,V,Fs,Pos1),
    ( []=Pos1 ->
        skolemise(X),
        Pos=[X]
    ;
        Pos=Pos1
    ).


find_possible_preexisting_values([],_V,_Fs, []).
find_possible_preexisting_values([O|Os],V,Fs,Pos) :-
    ( \+ \+ (V=O, coherent(Fs)) ->
          Pos = [O|Pos1]
    ; 
          Pos=Pos1
    ),
    find_possible_preexisting_values(Os,V,Fs,Pos1).
 
    
find_simultainously_consistent_assignment(Doms,Fs,Db1,Db2) :-
     append(Fs,Db1,Db2),
     assignment(Doms),
     coherent(Fs).
     
assignment([]).
assignment([domain(X,D)|Doms]) :-
    select(X,D,_D1),
    assignment(Doms).

\end{code}


\section{Dialog}

The proceedure mmm\_update needs to have a list of formulas we already know
about, as well as a list of objects we already know about.  Since we store
these in the form of asserted axiom/2 predicates, we'll need some routines to
assemble the list of formula and extract the known objects.

\begin{code}
assemble_formulas(Fs) :- findall(F,axiom(F),Fs).

extract_objects_from_formulas(Fs, Os2) :-
	extract_objects_from_formulas_aux(Fs,Os1),
	sort(Os1,Os2).

extract_objects_from_formulas_aux([],[]).
extract_objects_from_formulas_aux([F|Fs],Os3) :-
	extract_objects_from_formula(F,Os1),
	extract_objects_from_formulas_aux(Fs, Os2),
	append(Os1,Os2,Os3).

extract_objects_from_formula(F,Os) :- F=..[_|Os].
\end{code}

\noindent now we can write the dialog function:

\begin{code}
dialog :-
        % remove previous resuls
        retractall(axiom(_)),
	
        read_eval_loop.

read_eval_loop :-
        get_sentence(S),
        ( S=[bye,'.'] ->
            true
        ;
            process_sentence(S),
            read_eval_loop
        ).

process_sentence(S) :-
        % parse the sentence
        ( sentence(Mood,L,S,[]) ->
             % see how to process it
	    process_logical_form(Mood,L)
        ;
	    write('couldn\'t understand this:'),
	    writeln(S)
	).


% if this is an assertion, run the nute-covington transform and assert
process_logical_form(assertion,L) :-
	c_transform(L,Fs,Vs),
	assemble_formulas(Db1),
	retractall(axiom(_)),
        extract_objects_from_formulas(Db1,Os),
	mmm_update(Vs,Os,Fs,Db1,Db2),
	assert_all_as_axioms(Db2).

assert_all_as_axioms([]).
assert_all_as_axioms([F|Fs]) :-
	assert(axiom(F)),
	assert_all_as_axioms(Fs).

% if this is a query, run the lloyd-topor transform and query
process_logical_form(yn_question,L) :-
	lt_transform(L,Q),

	writeln(prove(Q,[])),
	
	(  prove(Q,[]) ->
	    writeln('yes')
	; 
            writeln('no')
	).
\end{code}


\chapter{The Sixth Chamber: Conjunction}

\label{language_game_and}

\section{Why we can't use unification}

Jack and Jill went up the hill. 

\section{Lambda Calculus}

\section{Developing Alpha Conversion and Beta Conversion using
Skeletons and Techniques}

This parser is the skeleton we use:

\subsection{Syntax of Lambda Expressions}

\begin{code}
*/

lambda_expression(E) :- var(E),!.

lambda_expression(F*A) :- !,
        lambda_expression_application(F,A).

lambda_expression(X^F) :-!,
        var(X),
        lambda_abstraction(X,F).

lambda_expression(E) :-
        E =.. [_F|Ts],
        map_lambda_expression(Ts).

lambda_expression_application(F,A) :- 
        lambda_expression(F),
        lambda_expression(A).

lambda_abstraction(_X,F) :-
        lambda_expression(F).

map_lambda_expression([]).
map_lambda_expression([T|Ts]) :-
        lambda_expression(T),
        map_lambda_expression(Ts).

/*
\end{code}

\subsection{Bound Variables}

To the above skeleton, we apply a techique.  The technique is to recognize
when a binding expression has been encountered, and save the variable which
has been bound in a bag.

So, the first thing we do is think about the bag we want to store the bound
variables in.  We'll put them in a list, but if we want to test if a variable
is a member off that list, we can't just use member/2, because member/2 will
unify.  So we write our own member-esque function which uses == instead of
unification:

\begin{code}
*/

variable_in_bag(X1,[X2|_]) :- X1 == X2.
variable_in_bag(X,[_|T]) :- variable_in_bag(X,T).

/*
\end{code}

Now for the visitor itself.  We rename ``lambdaExpression'' to
``visitBoundVariables'' and add an extra argument which holds, at any
point in the lambda expression, a list of the variables which are
bound.

\begin{code}
*/

visit_bound_variables(E, Bs) :- var(E),!,
        write(E), write(' is '),
        ( variable_in_bag(E,Bs) ->
            writeln(' bound')
        ;
            writeln(' free')
        ).

visit_bound_variables(F*A, Bs) :- !,
        visit_bound_variables_application(F,A, Bs).

visit_bound_variables(X^F, Bs) :-!,
        var(X),
        visit_bound_variable_abstraction(X,F,Bs).

visit_bound_variables(E, Bs) :-
        E =.. [_F|Ts],
        map_visit_bound_variables(Ts, Bs).

visit_bound_variables_application(F,A, Bs) :-!,
        visit_bound_variables(F, Bs),
        visit_bound_variables(A, Bs).

% notice that when a variable binder is encountered, it packs
% the variable on top of stack of bound variables
visit_bound_variable_abstraction(X,F,Bs) :-
        visit_bound_variables(F, [X|Bs]).

map_visit_bound_variables([], _Bs).
map_visit_bound_variables([T|Ts], Bs) :-
        visit_bound_variables(T, Bs),
        map_visit_bound_variables(Ts, Bs).

/*
\end{code}

\subsection{Alpha Conversion}

The bound variable visitor can in turn be used as a skeleton.  Alpha
conversion is the process of changing the names of the bound
variables.

The first step is to make a data structure which we can use to
associate one variable with another.  This is also derived using
skeletons and techniques; the skeleton being the variable\_in\_bag/2
procedure above.  Instead of just containing variables, the bag will
contain elements of the form sub(X,Y), which indicates that Y should
be substituted for X.

\begin{code}
*/

substitute_bound_variable(sub(X1,Y),[sub(X2,Y)|_]) :- X1 == X2.
substitute_bound_variable(X,[_|T]) :- substitute_bound_variable(X,T).

/*
\end{code}

We then add another parameter to the visit\_bound\_variables
predicate--an output parameter.  The alpha conversion process then
works as a transducer, walking over the input lambda expression,
substituting a fresh variable for every bound variable, and passing
everything else along unchnaged to the output parameter:

\begin{code}
*/

alpha_convert(F1, F2) :- alpha_convert(F1,[], F2).

alpha_convert(X1, Bs, X2) :- var(X1),!,
        ( substitute_bound_variable(sub(X1,X2),Bs) ->
            true
        ;
            X2=X1
        ).

alpha_convert(F1*A1, Bs, F2*A2) :- !,
        alpha_convert_application(F1,A1, Bs, F2,A2).

alpha_convert(X1^F1, Bs, X2^F2) :-!,
        var(X1),
        alpha_convert_abstraction(X1,F1, Bs, X2,F2).

alpha_convert(F1, Bs, F2) :-
        F1 =.. [Op|Ts1],
        map_alpha_convert(Ts1, Bs, Ts2),
        F2 =.. [Op|Ts2].

alpha_convert_application(F1,A1, Bs, F2,A2) :-
        alpha_convert(F1,Bs,F2),
        alpha_convert(A1,Bs,A2).

alpha_convert_abstraction(X1,F1, Bs, X2,F2) :-
        alpha_convert(F1, [sub(X1,X2)|Bs], F2).

map_alpha_convert([],_,[]).
map_alpha_convert([T1|Ts1], Bs, [T2|Ts2]) :-
        alpha_convert(T1,Bs,T2),
        map_alpha_convert(Ts1,Bs,Ts2).

/*
\end{code}

\subsection{Free Variables}

Free variables are variables appearing in a lambda expression which
arn't bound.  If we want to collect these, the easiest way is to also
collect the bound variables, and whenever we run across a variable,
check to see if it is a bound variable.  If its not, then its a free
variable, so we collect it.

So it would be plausible to use the boundVariable routine as a skeleton.
However, there's another consideration.  Consider the formula:
\begin{verbatim}
foo(X,X^bar(X))
\end{verbatim}
\noindent Should our predicate indicate that X is free or bound?
Its free in some contexts, and its bound in others.

The easiest (and safest) way out of this is to punt: Since the names
of bound variables are arbitrary, why not just use alpha conversion to
rename them?  That way, we're guarunteed that none of the variables
reported as free are bound in any context.

We could perform alpha conversion before collecting the free
variables, but then we'd have to make two passes over the input lambda
term.  This extra pass can be avoided simply by choosing as a skeleton
the alpha conversion predicate.

The technique is then to add a difference-list to the predicate
to collect the free variables.  The reason it is Perierran
instead of fregian is that the returned list of free variables
is the appendage of the lists from the subterms, so to get rid
of the overhad of appending we use different lists.

\begin{code}
*/

free_variables(F1, Us, F2) :- free_variables(F1, [], [],Us, F2).

free_variables(X1, Bs, U1,U2, X2) :- var(X1),!,
        ( substitute_bound_variable(sub(X1,X2),Bs) ->
            U1=U2
        ;
            X2=X1,
            U2=[X1|U1]
        ).

free_variables(F1*A1, Bs, U1,U2, F2*A2) :- !,
        free_variables_in_application(F1,A1, Bs, U1,U2, F2,A2).

free_variables(X1^F1, Bs, U1,U2, X2^F2) :-!,
        var(X1),
        free_variables_in_abstraction(X1,F1, Bs, U1,U2, X2,F2).

free_variables(F1, Bs, U1,U2, F2) :-
        F1 =.. [Op|Ts1],
        map_free_variables(Ts1, Bs, U1,U2, Ts2),
        F2 =.. [Op|Ts2].

free_variables_in_application(F1,A1, Bs, U1,U3, F2,A2) :-
        free_variables(F1, Bs, U1,U2, F2),
        free_variables(A1, Bs, U2,U3, A2).

free_variables_in_abstraction(X1,F1, Bs, U1,U2, X2,F2) :-
        free_variables(F1, [sub(X1,X2)|Bs], U1,U2, F2).

map_free_variables([], _, U,U, []).
map_free_variables([T1|Ts1], Bs, U1,U3, [T2|Ts2]) :-
        free_variables(T1, Bs, U1,U2, T2),
        map_free_variables(Ts1, Bs, U2,U3, Ts2).

/*
\end{code}

\subsection{Alphabetic Variants}

\begin{code}
*/

alphabetic_variants(A1,B1) :-
        % grab the free variables in both the terms
        free_variables(A1,FA,A2),
        free_variables(B1,FB,B2),
        
        % see if the free variables are the same
        FA == FB,
        
        % instantiate all of the variables.
        % note: we cant stipulate that N1 is the same as
        % N2, because if A2 and B2 share free variables,
        % they will be instantiated during the first numervars
        % call and the second will indicate a different number of
        % variables...
        numbervars(A2,0,_N1),
        numbervars(B2,0,_N2),
        
        % see if the resulting terms are the same
        A2=B2.

/*
\end{code}

\subsection{Beta Conversion}

Beta conversion is the process of applying beta reduction over and
over again until there's no more beta conversion which can be done.

\begin{code}

*/

% driver
beta_convert(X,Y) :- var(X),!, X=Y.

beta_convert(E,R) :- beta_convert(E, [], R).

beta_convert(X,_S,X) :- var(X),!.

beta_convert(F*A, S, R) :- !,
        alpha_convert(F,CF),
        beta_convert_application(CF,A, S, R).

beta_convert(X^F, S, R) :-!,
        var(X),
        beta_convert_abstraction(X,F, S, R).

beta_convert(E, S, R2) :-
        E =.. [F|Ts],
        map_beta_convert(Ts, Rs),
        R1 =..[F|Rs],
        reapply_stack(S,R1,R2).

reapply_stack([],R,R).
reapply_stack([A|S],R1,R2) :-
        reapply_stack(S,(R1*A),R2).

beta_convert_application(X,A, _S, X*A1) :- var(X),!,
        beta_convert(A,A1).

beta_convert_application(F,A, S, R) :- 
        beta_convert(F, [A|S], R).


beta_convert_abstraction(X,F, [], X^R) :-
        beta_convert(F,[],R).

beta_convert_abstraction(X,F, [X|S], R) :-
        beta_convert(F,S,R).


map_beta_convert([], []).
map_beta_convert([T|Ts], [R|Rs]) :-
        beta_convert(T, R),
        map_beta_convert(Ts, Rs).

/*
\end{code}


\section{Grammar}

\subsection{Left Recursion}

\begin{quote}
What's one and one and one and one and one and one and one? --the red queen,
in {\em Alice Through the Looking Glass}
\end{quote}

\begin{code}
*/

sentence(M,F) -->
    s(M,L),
    { beta_convert(L,F) }.


% the block is green.
s(assertion,NPL*VPL) --> 
    np(2,NPL),
    vp([],[], VPL),
    ['.'].

% there is a block.
s(assertion,NPL*VPL) --> 
    advp([],[],AvpF),
    verb([],[],W),
    np(2,NPL),
    vp([gap(W),gap(advp,AvpF)],[], VPL),
    ['.'].

% is the block green?
s(yn_question,NPL*VPL) --> 
    verb([],[],W),

    np(2,NPL), 

    vp([gap(W)],[], VPL),

    ['?'].

% is there a block?
s(yn_question,NPL*VPL) --> 
    verb([],[],W),
    advp([],[],AvpF),
    np(2,NPL), 
    vp([gap(W),gap(advp,AvpF)],[], VPL),
    
    ['?'].
    

np(C,D*N) --> 
     { C>0 },
     det(D), noun(N).

np(C,D*(X^and(A*X,N*X))) --> 
	{ C>0 },
	det(D), adjective(A), noun(N).

np(C,CL*N1*N2) -->
     { C>1 },
     np(1,N1),
     
     conjunction(CL),
     
     { C1 is C-1 },
     np(C1,N2).


det(R^Q^exists(X,and(R*X,Q*X)))--> [a].
det(R^Q^exists(X,and(R*X,Q*X)))--> [the].


noun(X^block(X)) --> [block].
noun(X^w_table(X)) --> [w_table].


% is green
vp(P1,P2,L) --> verb(P1,P2,_), adjective(L).

% is there
vp(P1,P3,L) --> verb(P1,P2,_), advp(P2,P3,L).


verb([],[],is) --> [is].
verb([gap(W)|P],P,W) --> [].


adjective(X^red(X)) --> [red].
adjective(X^green(X)) --> [green].


advp(P,P, F) --> adv(F).
advp([gap(advp,F)|P],P, F) --> [].


adv(_^true) --> [there].

% the whole point
conjunction(P^Q^X^and(P*X,Q*X)) --> [and].

/*
\end{code}

\chapter{The Seventh Chamber: Disjunction}


We want the language to be able to handle sentences like this:
\begin{verbatim}
The block is green or red.
The table is blue.
\end{verbatim}
\noindent and then ask quesitons like
\begin{verbatim}
Is the table blue?
Is the block red?
Is the block green or red?
\end{verbatim}
\noindent in other words, we want to be able to handle disjunctive 
adjective phrases.

In order to support this, we introduct some new productions for adjective
phrases which are very analogous to noun phrases.

\begin{code}
*/

sentence(M,F) -->
    s(M,L),
    { beta_convert(L,F) }.

s(assertion,NPL*VPL) --> 
    np(NPL),
    vp([],[], VPL),
    ['.'].

s(yn_question,NPL*VPL) --> 
    verb([],[],W),

    np(NPL), 

    vp([gap(W)],[], VPL),

    ['?'].

np(D*N) --> det(D), noun(N).

det(R^Q^exists(X,and(R*X,Q*X)))--> [a].

noun(X^block(X)) --> [block].
noun(X^w_table(X)) --> [w_table].

% new productions for adjective phrases
ap(C, L) --> {C>0}, adjective(L).

ap(C, CL*L1*L2) -->
	{ C>1 },
	ap(1,L1),
	
	conjunction(CL),
	
	{C1 is C-1},
	ap(C1,L2).

vp(P1,P2,L) --> verb(P1,P2,_), ap(2,L).

verb([],[],is) --> [is].
verb([gap(W)],[],W) --> [].

adjective(X^red(X)) --> [red].
adjective(X^green(X)) --> [green].

% looks funny to call "or" a conjunction,
% but in the grammar the names of the phrases
% refer to their grammatical categories, and
% the category of 'or' is conjunction.
conjunction(P^Q^X^or(P*X,Q*X)) --> [or].
/*
\end{code}

Now we only have to change the NC transform and the FT transforms and we're
done. 

\section{The problem of representing disjunctive information in prolog}

A prolog program describes one world, its least Herbrand model.  But a
disjuntive formula can have many models.  How to cope?  With a modal metaphor.

\section{Theorem Prover}

\subsection{Hypothetical Reasoning}

\begin{code}
*/

% prove((A,B), R1,R3) :-
%    prove(A, R1,R2),
%    prove(B, R2,R3).

prove(A, R,R) :- axiom(A).

prove(A, R,R) :- 
      dial(A,N,V),
      member(N=V,R),!.

prove(A, R,[N=V|R]) :-
    dial(A,N,V),
    not member(N=_,R).

/*
\end{code}

\subsection{Proving Disjunctive Queries}

So how do you go about demonstrating that a statement like
\begin{verbatim}
or(block(X),table(X))
\end{verbatim}
\noindent is true?  One way is to actually find an object which is a block,
or, alternately, find an object which is a table.  If you can exhibit such an
object, you've proven the disjunction.  Its very straightforward to enhance
the theorem prover to reason this way:

\begin{code}

prove((A ; B), R1,R2) :-
    (
        prove(A, R1,R2)
    ;
        prove(B, R1,R2)
    ).
    
prove((A,B), R1,R3) :-
    prove(A, R1,R2),
    prove(B, R2,R3).

prove(A, R,R) :- axiom(A).

prove(A, R,R) :- 
      dial(A,N,V),
      member(N=V,R),!.

prove(A, R,[N=V|R]) :-
    dial(A,N,V),
    not member(N=_,R).
\end{code}

However, you don't actually have to be able to find a block or a table to
prove:
\begin{verbatim}
or(block(X),table(X))
\end{verbatim}
\noindent There's another way.  Suppose you can find a set of 
asumptions such that
\begin{enumerate}
\item the assumptions are {\em exhaustive}, that is, at least one of them
must be true.
\item if under each of those assumptions, you can find a block or a
table
\end{enumerate}
\noindent then you've also proved your case.  It doesn't even have to be the
same block or the same table for each assumption, just so long as for every
possible way the world could be, there is a block or a table, that's all you
need to show.
\begin{code}
prove1((A ; B), R1,R2):-
        findall(R, 
                (
                  prove(A ,R1,R)
                ;
                  prove(B, R1,R)
                ),
                Rs1),
         sort(Rs1,Rs2),
         filter_exhaustive_switches(Rs2,Rs3),
         member(R2,Rs3).


filter_exhaustive_switches([],[]).
filter_exhaustive_switches([Rs|Rs1],Rs2) :-
     % use first element as loop control.
     switch_to_property_list(Rs,Ps1),
     gather_switches_which_appear_in_all_residues(Rs1, Ps1,Ps2),
     sort_property_list_values(Ps2,Ps3),
     find_switches_which_every_value_appears(Ps3,Ss).


find_switches_which_every_value_appears([],[]).
find_switches_which_every_value_appears([N-Vs|Ps],Ss) :-
     all_switch_values(N,Ves),
     (Vs=Ves ->
         Ss = [N|Ss1]
     ;
         Ss = Ss1
     ),
     find_switches_which_every_value_appears(Ps,Ss1).

% changes a list of the form [n1=v1, n2=v2, ...] 
% to a list of the form [n1-[v1], n2-[v2], ...]
switch_to_property_list([],[]).
switch_to_property_list([N=V|Ss], [N-[V]|Ps]) :-
        switch_to_property_list(Ss, Ps).


% if a switch isn't a member of _all_ of the  residues, it is eliminated 
% from the list. if it _is_ a member of all of the residues, this predicate 
% groups all of the switch values found in the list for the switch.
sort_property_list_values([],[]).
sort_property_list_values([N-Vs1|Ps1],[N-Vs2|Ps2]) :-
    sort(Vs1,Vs2),
    sort_property_list_values(Ps1,Ps2).

gather_switches_which_appear_in_all_residues([], Ps,Ps).
gather_switches_which_appear_in_all_residues([R|Rs], Ps1,Ps3) :-
        determine_whether_switch_appears_in_residue(Ps1, R, Ps2),
        gather_switches_which_appear_in_all_residues(Rs, Ps2,Ps3).
        
determine_whether_switch_appears_in_residue([],_R, []).
determine_whether_switch_appears_in_residue([N-Vs|Ps1],R1,Ps3) :-
        ( select(N=V, R1,R2) ->
            Ps3 = [N-[V|Vs] | Ps2]
        ;
            Ps3 = Ps1
        ),
        determine_whether_switch_appears_in_residue(Ps1, R2, Ps2).


prove((A,B), R1,R3) :-
    prove(A, R1,R2),
    prove(B, R2,R3).

prove(A, R,R) :- axiom(A).

prove(A, R,R) :- 
      dial(A,N,V),
      member(N=V,R),!.

prove(A, R,[N=V|R]) :-
    dial(A,N,V),
    not member(N=_,R).

dial(_,_,_) :- fail.
\end{code}

\subsubsection{Nute-Covington Transform}

\begin{code}
*/
nc_transform(F,Cs) :- nc_transform(F,Cs,[]).

nc_transform(exists(X,R), H,T) :- !,
        skolemise(X),
        nc_transform(R,H,T).

nc_transform(and(R,S), H,T) :-!,
        nc_transform(R, H,I),
        nc_transform(S, I,T).

nc_transform(or(R,S), H,T) :-!,
        skolemise(N),
        
        nc_transform(R, R1,[]),
        skolemise(V1),
        make_switch_value(R1, N,V1, H,I),
        
        nc_transform(S, S1,[]),
        skolemise(V2),
        make_switch_value(S1, N,V2, I,T).


make_switch_value([], _N,_V, L,L).
make_switch_value([axiom(R)|Rs], N,V, [dial(R,N,V)|I],T) :-
        make_switch_value(Rs, N,V, I,T).


nc_transform(F, [axiom(F)|T],T).
/*
\end{code}

\section{Compling for query}

\subsection{Lloyd-Topor Transform}

\begin{code}
*/

lt_transform(exists(_X,R), R1) :- !,
        lt_transform(R,R1).

lt_transform(and(P,Q), (P1,Q1)) :- !,
        lt_transform(P,P1),
        lt_transform(Q,Q1).

lt_transform(or(P,Q), (P1 ; Q1)) :- !,
        lt_transform(P,P1),
        lt_transform(Q,Q1).

lt_transform(F,F).

/*
\end{code}

\section{Dialog}

\subsection{Read-Eval Print Loop}

\begin{code}
*/

dialog :-
        % remove previous resuls
        retractall(axiom(_)),
        retractall(xor(_,_,_)),
        
        read_eval_loop.

read_eval_loop :-
        get_sentence(S),
        ( S=[bye,'.'] ->
            true
        ;
            process_sentence(S),
            read_eval_loop
        ).

process_sentence(S) :-
        % parse the sentence
        ( sentence(Mood,L,S,[]) ->
             % see how to process it
             process_logical_form(Mood,L)
        ;
             write('couldn\'t understand this:'),
             writeln(S)
        ).

/*
\end{code}

\subsection{Handling assertions}

\begin{code}
*/

% if this is an assertion, run the nute-covington transform and assert
process_logical_form(assertion,L) :-
    nc_transform(L,Fs),
    assert_all(Fs).

assert_all([]).
    
assert_all([F|Fs]) :-
    assert(F),
    assert_all(Fs).


/*
\end{code}

\subsection{Handling Queries}

Queries needs to be beefed up, however.  We still use the Lloyd-Topor
transforms to get a logical formula for us to prove.  However, "yes" and "no"
arn't the only answers to yes/no questions anymore, not when we have
disjunctive knowledge.  The apropos answer might be "maybe".

When is maybe the apropriate answer?  Suppose we can prove F, but only by
using some assumptions.  Further suppose that upon other assumptions, or with
no assumptions, we can't prove F.  The F may be true, depending upon what else
might be true.  Therefore, the apropos answer is "maybe".

Its pretty obvious when the answer should be no--we can't prove F no matter
what assumptions we make or don't make.  But when can we confidently answer
"yes?"  We can answer yes only when, no matter what assumptions we make or
don't make, we can {\em always} prove F.

There are two conditions under which this happens:  if
\begin{verbatim}
prove(F,[],[])
\end{verbatim}
\noindent suceeds, then we know that F is true no matter what additional
assummptions are made.  But suppose something like:
\begin{verbatim}
prove(F,[],[switch1=on,switch2=off])
\end{verbatim}
\noindent succeeds.  We can only answer "yes" (and not "maybe") if all four
combinations for the values of switch1 and switch2 we can still prove F.

\begin{code}

% if this is a query, run the Lloyd-Topor transform and query
process_logical_form(yn_question,L) :-
    lt_transform(L,Q),
        
    findall(R,prove(Q,[],R), Rs),
    
    determine_correct_answer(Rs,Q).

determine_correct_answer([], _Q) :- !, writeln('no').

determine_correct_answer(Rs,Q) :-
    find_which_switches_are_used(Rs,Ss),
    
    ( prove_for_every_switch_value(Q,Ss) ->
        writeln('yes')
     ; 
        writeln('maybe')
    ).

find_which_switches_are_used(Rs, S) :-
        find_which_switches_are_used(Rs, S1,[]),
        sort(S1,S).

find_which_switches_are_used([], S,S).
find_which_switches_are_used([R|Rs], S1,S3) :-
        find_switches_for_residue(R, S1,S2),
        find_which_switches_are_used(Rs, S2,S3).

find_switches_for_residue([],S,S).
find_switches_for_residue([N=_V|R],[N|S1],S2) :-
        find_switches_for_residue(R,S1,S2).

prove_for_every_switch_value(Q,Ss) :-
        not (not (possible_switch_setting(Ss, R),
                 prove(Q,R,R))).

possible_switch_setting([],[]).
possible_switch_setting([S|Ss],[V|Vs]) :-
       xor(_F,S,V),
       possible_switch_setting(Ss,Vs).

\end{code}

\section{The Staggering Inefficiencies of Reasoning Disjunctively}

Of course, a loop like:
\begin{verbatim}
not (not (possible_switch_setting(Ss,R), prove(Q,R,R)))
\end{verbatim}
\noindent makes it painfully obvious that for any reasonable number of
switches, the runtime will simply explode.  If you are wondering why people
find it so hard to imagine the world in a different way, to visualise world
peace, this is the reason.  

This problem is amelieorated by several things.  First, computer power is
still growing exponentially.  If you wonder what we'll be doing with 64-core
microprocessors, there's your answer.  But even today's processors can run
thorugh all $2^{32}$ possibilities of a 32-bit vector in about a second or so.
So its not hopeless.

Still, as a general strategy, its best to stick with definite knowledge as
much as possible.  We'll call this the {\em Autistic Heuristic}.

\chapter{The Eighth Chamber: Conjunction and Disjunction}

\chapter{The Ninth Chamber: Universal Quantification}

\chapter{The Tenth Chamber: Negation}

\chapter{The Eleventh Chamber: Equality and Functions}

\chapter{The Twelfth Chamber: Prepositional Phrases}

\chapter{The Thirteenth Chamber: Building a better NP}

\chapter{The Fourteenth Chamber: Building a better VP}

\chapter{The Fifteenth Chamber: Neo-Davidsonian Events}

\chapter{The Sixteenth Chamber: Cause, Effect, and The Frame Problem} 

\chapter{The Seventeenth Chamber: The Subjunctive Mood}

\chapter{The Eigthteenth Chamber: Tense}

\chapter{The Nineteenth  Chamber: Pronouns and Anaphora}

\chapter{The Twentyeth Chamber: Numbers and Plurals}

\chapter{The Twenty-firstChamber: Spatial Reasoning}

\chapter{The Twenty-second Chamber: Planning}

\chapter{The Twenty-third Chamber: Belief}

\chapter{The Twenty-fourth Chamber: Multiple Agents}

\chapter{The Twenty-fifth Chamber: Degrees of Belief}

\chapter{The Twenty-sixth Chamber: Presupposition}

\chapter{The Twenty-seventh Chamber: Conversational Implicature}

\chapter{The Twenty-eighth Chamber: Natural Lanugae Generation}

\chapter{The Twenty-eighth Chamber: Dialog}

\chapter{The Twenty-ninth Chamber: }

\chapter{The Thirtyeth Chamber: }

\chapter{The Thirty-first Chamber: }

\chapter{The Thirty-second Chamber: }

\chapter{The Thirty-third Chamber: }

\chapter{The Thirty-fourth Chamber: }

\chapter{The Thirty-fifth Chamber: }

\chapter{The Thirty-sixth Chamber: }


\backmatter

\addcontentsline{toc}{chapter}{Bibliography}
\begin{thebibliography}{1}
\bibitem{del8}
\newblock The Fracas Consortium, Robin Cooper, et al.
\newblock Describing the Approaches
\newblock The Fracas Project deliverable D8
\newblock Available online, just google for it...
\newblock December 1994
\bibitem{del9}
\newblock The Fracas Consortium, Robin Cooper, et al.
\newblock The State of the Art in Computational Semantics:
\newblock Evaluating the Descriptive Capabilities of Semantic Theories
\newblock The Fracas Project deliverable D9
\newblock Available online, just google for it...
\newblock December 1994
\bibitem{keller88}
\newblock William Ralph (Bill) Keller.
\newblock Nested Cooper storage: The proper treatment of
lquantification in ordinary noun phrases.
\newblock In U. Reyle and C. Rohrer, editors
\newblock Natural Language parsing and Linguistic Theories
\newblock pages 432-445
\newblock Reidel, Dordrecht, 1988.
\bibitem{larson85}
\newblock Larson, R.K. (1985).  On the syntax of disjunction scope.
\newblock Natural Language and Linguistic Theory
\newblock 3:217, 265
\bibitem{nasslli2003}
\newblock Patrick Blackburn and Johan Bos
\newblock Computational Semantics for Natural Language
\newblock Course Notes for NASSLLI 2003
\newblock Indiana University, June 17-21, 2003
\newblock Available on the web, just google for it....
\bibitem{marcus93}
\newblock Mitchell P. Marcus,  Beatrice Santorini and Mary ann Marciniewicz.
\newblock Building a large annotated corpus of English: the Penn Treebank.
\newblock in Computational Linguistics, vol. 19, 1993.
\bibitem{Parsons90}
\newblock Terrence Parsons
\newblock Events in the semantics of English
\newblock MIT press, Cambridge, Ma.
\newblock 1990
\bibitem{pereira87}
\newblock Fernando C.N. Pereira and Stuart M. Shieber
\newblock Prolog and Natural Language analysis
\newblock CSLI lecture notes no. 10
\newblock Stanford, CA
\newblock 1987
\end{thebibliography}

\end{document}

*/
